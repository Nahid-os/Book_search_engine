{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b5ccf-93e7-4ed3-9052-4753d160cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) Streams interactions from MongoDB in batches\n",
    "2) Builds a sparse book×user CSR matrix\n",
    "3) Runs TruncatedSVD to get low-dim embeddings\n",
    "4) Builds a Faiss index for cosine similarity\n",
    "5) For each book, finds top-K similar books and writes them\n",
    "   into MongoDB under the `similar_books` field.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# ─────────────────────────────── CONFIG ────────────────────────────────────────\n",
    "MONGO_URI      = os.getenv(\"MONGO_URI\", \"mongodb://127.0.0.1:27017\")\n",
    "DB_NAME        = \"book_database\"\n",
    "INTER_COLL     = \"goodreads_interactions\"\n",
    "BOOKS_COLL     = \"books\"\n",
    "BATCH_SIZE     = 2_000_000           # docs per batch when streaming\n",
    "EMBED_DIM      = 50                  # SVD embedding size\n",
    "FAISS_INDEX_TYPE = \"Flat\"            # \"Flat\" or \"HNSW\"\n",
    "TOP_K          = 10                  # how many neighbors to store\n",
    "BULK_BATCH     = 5_000               # mongo bulk-write batch size\n",
    "\n",
    "# ──────────────────── STEP 1: STREAM & BUILD SPARSE MATRIX ────────────────────\n",
    "def stream_and_build_matrix():\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    coll   = client[DB_NAME][INTER_COLL]\n",
    "\n",
    "    # 1a) discover all unique users & books\n",
    "    users = set(); books = set()\n",
    "    for doc in coll.find({}, {\"user_id\":1,\"book_id\":1}, batch_size=BATCH_SIZE):\n",
    "        users.add(doc[\"user_id\"])\n",
    "        books.add(doc[\"book_id\"])\n",
    "    user2idx = {u:i for i,u in enumerate(sorted(users))}\n",
    "    book2idx = {b:i for i,b in enumerate(sorted(books))}\n",
    "    idx2book = {i:b for b,i in book2idx.items()}\n",
    "    n_users, n_books = len(user2idx), len(book2idx)\n",
    "    print(f\"→ Found {n_books:,} books & {n_users:,} users.\")\n",
    "\n",
    "    # 1b) stream again into COO arrays\n",
    "    rows, cols, data = [], [], []\n",
    "    count = 0\n",
    "    for doc in coll.find({}, {\"user_id\":1,\"book_id\":1,\"rating\":1}, batch_size=BATCH_SIZE):\n",
    "        u = doc[\"user_id\"]; b = doc[\"book_id\"]\n",
    "        rows.append(book2idx[b])\n",
    "        cols.append(user2idx[u])\n",
    "        data.append(float(doc.get(\"rating\", 1.0)))\n",
    "        count += 1\n",
    "        if count % BATCH_SIZE == 0:\n",
    "            print(f\"  streamed {count:,} interactions…\")\n",
    "    client.close()\n",
    "    print(f\"→ Total interactions: {count:,}\")\n",
    "\n",
    "    mat = coo_matrix((data, (rows, cols)), shape=(n_books, n_users)).tocsr()\n",
    "    return mat, idx2book\n",
    "\n",
    "# ─────────────────── STEP 2: TRUNCATED SVD FOR EMBEDDINGS ─────────────────────\n",
    "def compute_embeddings(mat, n_components=EMBED_DIM):\n",
    "    print(f\"→ Running TruncatedSVD to {n_components} dims…\")\n",
    "    svd = TruncatedSVD(n_components=n_components, algorithm=\"arpack\", random_state=42)\n",
    "    emb = svd.fit_transform(mat)  # shape (n_books, n_components)\n",
    "    print(f\"   explained variance: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "    return emb\n",
    "\n",
    "# ────────────────── STEP 3: BUILD FAISS COSINE INDEX ───────────────────────────\n",
    "def build_faiss_index(emb, index_type=FAISS_INDEX_TYPE):\n",
    "    d = emb.shape[1]\n",
    "    faiss.normalize_L2(emb)\n",
    "    if index_type == \"Flat\":\n",
    "        idx = faiss.IndexFlatIP(d)\n",
    "    elif index_type == \"HNSW\":\n",
    "        idx = faiss.IndexHNSWFlat(d, 32)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown index type\")\n",
    "    idx.add(emb)\n",
    "    print(f\"→ Faiss index built (ntotal={idx.ntotal})\")\n",
    "    return idx\n",
    "\n",
    "# ──────────────── STEP 4: PRECOMPUTE & BULK-WRITE SIMILARS ─────────────────────\n",
    "def precompute_and_write(index, emb, idx2book, top_k=TOP_K):\n",
    "    book2idx = {b:i for i,b in idx2book.items()}\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    coll   = client[DB_NAME][BOOKS_COLL]\n",
    "    ops    = []\n",
    "\n",
    "    def get_top_k(bid):\n",
    "        i = book2idx[bid]\n",
    "        D, I = index.search(emb[i : i+1], top_k+1)\n",
    "        sims = []\n",
    "        for dist,idx in zip(D[0], I[0]):\n",
    "            if idx == i: continue\n",
    "            sims.append(idx2book[idx])\n",
    "            if len(sims) >= top_k: break\n",
    "        return sims\n",
    "\n",
    "    print(\"→ Computing & queuing updates…\")\n",
    "    for i, bid in idx2book.items():\n",
    "        sim_ids = get_top_k(bid)\n",
    "        ops.append(UpdateOne({\"book_id\": bid},\n",
    "                             {\"$set\": {\"similar_books\": sim_ids}}))\n",
    "        if len(ops) >= BULK_BATCH:\n",
    "            coll.bulk_write(ops)\n",
    "            print(f\"   wrote {len(ops)} docs…\")\n",
    "            ops = []\n",
    "    if ops:\n",
    "        coll.bulk_write(ops)\n",
    "        print(f\"   wrote final {len(ops)} docs.\")\n",
    "    client.close()\n",
    "    print(\"→ All books updated with `similar_books`.\")\n",
    "\n",
    "# ───────────────────────────── MAIN ────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) build matrix\n",
    "    mat, idx2book = stream_and_build_matrix()\n",
    "\n",
    "    # 2) compute embeddings\n",
    "    embeddings = compute_embeddings(mat)\n",
    "\n",
    "    # 3) build Faiss index\n",
    "    faiss_idx = build_faiss_index(embeddings)\n",
    "\n",
    "    # 4) precompute & write\n",
    "    precompute_and_write(faiss_idx, embeddings, idx2book)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
